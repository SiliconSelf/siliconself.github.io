+++
title =  "If Your Monolith Sucks, Your Microservices Won't Be Any Better"
tags = ["software design"]
date = "2023-12-27"
draft = true
+++
I recently had a conversation with an ops friend about the trend of using microservices for everything. If you don't have time to read this entire post, the general conclusion is that using microservices will not save you from the consequences of choosing the wrong tools for the job at hand.

Microservices for the sake of microservices is really not all what it's cracked up to be. We were promised smaller, interconnected projects with better scalability and lower cognitive complexity allowing for easier maintainability. What we got instead was Kubernetes, ever-increasing AWS bills with services like Lambda and EKS, and further apologetics for the use of JavaScript on the backend for higher and higher demand applications where JavaScript is increasingly out of place. This isn't to say microservice architectures don't have their strengths. After all, even if I have criticism it's not like FAANG companies developed them for no reason. If enough people are accessing a small part of your overall service, it totally makes sense to break it out into a smaller microservice so those people aren't slowing down the broader scope.

Unfortunately, this reasoning has been used to justify lack of developer skill and poor choices in tools and design. When you create a new Cloudflare Worker or a new function in AWS Lambda, it's assumed you'll be using JavaScript to do it. It's what their editors default to to and, in the case of Lambda, using anything else like Rust comes with significant extra work for things like setting up a different environment. Treating a slow, single threaded, interpreted language like JavaScript (or even worse, Python) as a default also means treating the overhead and wasted hardware capacity implied by using those tools as default. When that lost potential is forgotten, the threshold for when a service can't keep up with demand anymore gets a lot lower. When the threshold gets lower, systems like Kubernetes get introduced a lot sooner. Messaging-heavy architectures are planned from the start because people know subconsciously that the language and tools they're building this service in won't be able to keep up with demand. Then, when things fail, tracing has to dive into all the abstraction layers Kubernetes has put in place. Is the problem with the apps in containers? The network? Network abstraction? Something completely different? Something in the deployment metadata? Tons of extra abstraction layers and complexity gets added instead of using better tools to reduce the baseline lost potential.

Nowhere is this more glaring than charts like this one by [@lholznagel](https://medium.com/@lholznagel/comparing-nodejs-and-rust-http-frameworks-response-times-5738dfa1843d), which shows even the fastest backend JavaScript framework losing to a completely unoptimized build of the same logic in Rocket. When the Rust approach has its usual optimizations turned on, the JavaScript frameworks aren't even in the same order of magnitude of performance anymore.

![Chart](/images/rocketexpress.webp)

If you picked the wrong tools from the start and built your monolith poorly, microservices won't save you. At a certain level, you just have to write better code and use better tools. Any gains you get from using messaging-heavy microservices architectures can typically be dwarfed by the gains from using a language that isn't Node, Python, or Ruby on Rails. Stop accepting worse performance from your tools just because there's some hot trend to make up for it. Use better tools. Write better code.